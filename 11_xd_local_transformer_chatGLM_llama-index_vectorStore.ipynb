{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全部采用本地化部署的方式和开源模型来建立的问答能力和本地化知识库集成的能力"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaodong/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.c -shared -o /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.so\n",
      "Load kernel : /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 6\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照langchain标准进行模型封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from llama_index import LLMPredictor\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response, history = model.chat(tokenizer, prompt, history=[])\n",
    "        # only return newly generated tokens\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"name_of_model\": \"THUDM/chatglm-6b-int4\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入embedding模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义包含embedding模型和语言模型的Service Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建本地知识库（本地知识的索引库）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 130, which is longer than the specified 100\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import GPTVectorStoreIndex\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=100, chunk_overlap=20)\n",
    "parser = SimpleNodeParser(text_splitter=text_splitter)\n",
    "documents = SimpleDirectoryReader('./data/faq/').load_data()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "\n",
    "index = GPTVectorStoreIndex(nodes=nodes, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合本地知识库和语言模型进行提问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 我们的配送范围是全国大部分省份，包括北京、上海、天津、重庆、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆。因此，天津是我们可以配送的地区之一。至于退货政策，请参考我们的用户手册，每个订单的情况可能不同，因此具体的政策可能会因订单而异。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 你们能配送到天津吗？另外，你们的退货政策是怎样的？\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLM-6B 是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型 GLM-6B 开发的，我的任务是针对用户的问题和要求提供适当的答复和支持。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 请介绍一下ChatGLM？\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca是一个基于深度学习的自然语言处理模型，由Google开发。它被设计为能够处理自然语言，并能够像人类一样理解、阅读、翻译和生成文本。Alpaca采用了与BERT类似的架构，但是与BERT不同的是，BERT是为了解决语言模型在处理长句子和多任务学习上的问题，而Alpaca的目标是在大规模数据集上实现高效的自然语言理解和生成。\n",
      "\n",
      "Alpaca采用了多种技术，包括上下文无关文法(Context-Free Language)、词向量(Word Embeddings)、自注意力机制(Self-Attention)、和基于Transformer架构的编码器-解码器模型。这些技术使得Alpaca能够在处理大规模文本数据集时表现出色，如处理维基百科、新闻报道、电子邮件等数据集时，取得了良好的效果。\n",
      "\n",
      "Alpaca的应用广泛，已经被应用于自然语言处理、机器翻译、文本生成、智能客服等领域。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 请介绍一下语言模型中的Alpaca。\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca 是一个开源的语言模型，可以在本地 GPU 上运行。以下是一个 Python 代码示例，展示了如何使用 Alpaca 模型在本地 GPU 上运行支持语言任务的能力：\n",
      "```python\n",
      "from alpaca_model_manager import ModelManager\n",
      "\n",
      "# 创建 Alpaca 模型\n",
      "manager = ModelManager()\n",
      "\n",
      "# 获取 Alpaca 模型的参数\n",
      "model_params = manager.get_model_params()\n",
      "\n",
      "# 获取模型的权重\n",
      "model_weights = model_params['weights']\n",
      "\n",
      "# 使用模型训练语言任务\n",
      "model_manager.train_text(\n",
      "    text_dataset,\n",
      "    text_model,\n",
      "    model_weights,\n",
      "    num_epochs=10,\n",
      "    data_dir='/path/to/local/data',\n",
      "    data_per_device=1,\n",
      "    device='GPU'\n",
      ")\n",
      "```\n",
      "在这个示例中，我们首先创建了一个 Alpaca 模型的实例，然后获取了模型的参数和权重。接着，我们使用模型训练语言任务，将训练数据移动到本地 GPU 上，并设置数据访问权限。最后，我们使用模型的参数和权重对语言任务进行训练。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 请写一段python代码，展示如何使用Alpaca的模型在本地GPU运行支持语言任务的能力。\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用新的TextSplitter来对原始内容进行处理，换一个新的index来处理索引，对文章进行总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "new_text_splitter = SpacyTextSplitter(pipeline=\"zh_core_web_sm\", chunk_size = 2048)\n",
    "new_parser = SimpleNodeParser(text_splitter=new_text_splitter)\n",
    "new_documents = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "new_nodes = new_parser.get_nodes_from_documents(new_documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1680 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这段文字是鲁迅先生的《藤野先生》节选。以下是对这段文字的总结：\n",
      "\n",
      "我还记得东京的樱花烂熳的时节，我去了仙台医学专门学校。开始在学生宿舍里住，后来搬到了一个远离监狱的地方，每天过得并不怎么样，但是遇到了一些有趣的人物和经历。\n",
      "\n",
      "我一开始在学生宿舍里住，那里很热，蚊子很多，但我每天过得并不怎么样，但遇到了一些有趣的人物。我遇到了一个中国的留学生，我们一起上课、考试，还去喝酒。\n",
      "\n",
      "后来，我搬到了一个远离监狱的地方，每天过得并不怎么样，但遇到了一些有趣的人物。我遇到了一些留学生，我们一起上课、考试，还去喝酒。\n",
      "\n",
      "我虽然遇到了很多有趣的人物，但是最终也明白了一些事情。我最终明白了藤野先生的穿衣风格，以及他对学生的严格要求。我也明白了学习的重要性，以及在困难时期要坚持的信念。\n",
      "\n",
      "这段文字是鲁迅先生在回忆自己的学运经历，讲述了自己在中国的一次学运中的经历，以及对自己的影响。在这段文字中，作者描绘了自己与同学一起讨论中国女性裹脚的问题，被老师误解，被同学嘲笑，但最终还是理解了老师和同学的心情，感受到了他们对中国的热爱。作者也描述了自己参观枪毙中国人的命运，被中国人欢呼声所刺耳，感受到了中国弱国的事实，以及日本战胜俄国的情形。作者还回忆了与藤野先生的对话，表达了自己对藤野先生对中国传统文化的热爱和尊重，以及藤野先生对自己的误解和失望。整段文字表达了作者对中国传统文化的热爱和对自己成长经历的思考。\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTListIndex\n",
    "\n",
    "list_index = GPTListIndex(nodes=new_nodes, service_context=service_context)\n",
    "\n",
    "# response = list_index.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\", response_mode=\"tree_summarize\")\n",
    "\n",
    "query_engine = list_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "response = query_engine.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换用TreeIndex来试试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲁迅在日本仙台医学专门学校读书时的老师藤野先生是一位来自日本的解剖学教授，他让我们在学生之间轮流抄他讲义，并在最后让我们把讲义送给他。他对我们的教育很用心，但我们却经常不用功，有些任性。最后，他的讲义被我们全部推翻，重新用红笔添改过，并达到了他想要的效果。\n",
      "\n",
      "藤野先生还让我们在学生之间轮流抄他讲义，并最后让我们把讲义送给他。虽然我们都有些任性，但他对我们的教育很用心，对我们的教育很用心，但我们却经常不用功，有些任性。最后，他的讲义被我们全部推翻，重新用红笔添改过，并达到了他想要的效果。\n",
      "\n",
      "学年试验完毕之后，鲁迅去了东京玩一夏天，秋初再回学校，成绩早已发表了，同学一百余人之中，他在中间，不过是没有落第。这回藤野先生所担任的功课，是解剖实习和局部解剖学。\n",
      "\n",
      "藤野先生也偶有使我很为难的时候。他听说中国的女人是裹脚的，但不知道详细，所以要问我怎么裹法，足骨变成怎样的畸形，还叹息道，“总要看一看才知道。\n",
      "\n",
      "究竟是怎么一回事呢？”\n",
      "\n",
      "有一天，本级的学生会干事到我寓里来了，要借我的讲义看。鲁迅检出来交给他们，却只翻检了一通，并没有带走。但他们一走，邮差就送到一封很厚的信，打开后，第一句是：“你改悔罢！”这是《新约》上的句子，但经托尔斯泰新近引用过的。\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTTreeIndex\n",
    "\n",
    "# define LLM\n",
    "tree_index = GPTTreeIndex(nodes=new_nodes, service_context=service_context)\n",
    "# response = tree_index.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\", mode=\"summarize\")\n",
    "\n",
    "#query_engine = tree_index.as_query_engine(\n",
    "#    response_mode=\"summarize\"\n",
    "#)\n",
    "\n",
    "query_engine = tree_index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试Vector Store - pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 目前这个不分不工作\\nimport pinecone\\nfrom llama_index import StorageContext\\nfrom llama_index.vector_stores import PineconeVectorStore\\nimport os\\n\\n# pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\\n# pinecone_env = os.environ.get(\"PINECONE_ENV\")\\n\\npinecone_api_key = os.environ[\\'PINECONE_API_KEY\\']\\npinecone_env = os.environ[\\'PINECONE_ENV\\']\\n\\n\\n# init pinecone\\npinecone.init(pinecone_api_key, pinecone_env)\\npinecone.create_index(\"xdtest1\", dimension=1536, metric=\"cosine\", pod_type=\"p1\") # pinecone.create_index(\"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")\\n\\n# construct vector store and customize storage context\\nstorage_context = StorageContext.from_defaults(\\n    vector_store = PineconeVectorStore(pinecone.Index(\"xdtest1\"))\\n)\\n\\n# Load documents and build index\\ndocuments_p = SimpleDirectoryReader(\\'./data/mr_fujino\\').load_data()\\nindex_p = GPTVectorStoreIndex.from_documents(documents_p, service_context=service_context, storage_context=storage_context)\\n\\nquery_engine = index_p.as_query_engine()\\n\\nresponse = query_engine.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\")\\n\\nprint(response)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 目前这个不分不工作\n",
    "import pinecone\n",
    "from llama_index import StorageContext\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "import os\n",
    "\n",
    "# pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "# pinecone_env = os.environ.get(\"PINECONE_ENV\")\n",
    "\n",
    "pinecone_api_key = os.environ['PINECONE_API_KEY']\n",
    "pinecone_env = os.environ['PINECONE_ENV']\n",
    "\n",
    "\n",
    "# init pinecone\n",
    "pinecone.init(pinecone_api_key, pinecone_env)\n",
    "pinecone.create_index(\"xdtest1\", dimension=1536, metric=\"cosine\", pod_type=\"p1\") # pinecone.create_index(\"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")\n",
    "\n",
    "# construct vector store and customize storage context\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store = PineconeVectorStore(pinecone.Index(\"xdtest1\"))\n",
    ")\n",
    "\n",
    "# Load documents and build index\n",
    "documents_p = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "index_p = GPTVectorStoreIndex.from_documents(documents_p, service_context=service_context, storage_context=storage_context)\n",
    "\n",
    "query_engine = index_p.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\")\n",
    "\n",
    "print(response)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss Vector Store的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: GanymedeNil/text2vec-large-chinese\n",
      "Load pretrained SentenceTransformer: GanymedeNil/text2vec-large-chinese\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /home/xiaodong/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name /home/xiaodong/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "Successfully loaded faiss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import UnstructuredFileLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "em_model = \"GanymedeNil/text2vec-large-chinese\"\n",
    "em_device = \"cuda\"\n",
    "\n",
    "hgf_embeddings = HuggingFaceEmbeddings(model_name=em_model,\n",
    "                                        model_kwargs={'device': em_device})\n",
    "\n",
    "import re\n",
    "\n",
    "# 文本分句长度\n",
    "SENTENCE_SIZE = 100\n",
    "\n",
    "class ChineseTextSplitter(CharacterTextSplitter):\n",
    "    def __init__(self, pdf: bool = False, sentence_size: int = SENTENCE_SIZE, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pdf = pdf\n",
    "        self.sentence_size = sentence_size\n",
    "\n",
    "    def split_text1(self, text: str) -> List[str]:\n",
    "        if self.pdf:\n",
    "            text = re.sub(r\"\\n{3,}\", \"\\n\", text)\n",
    "            text = re.sub('\\s', ' ', text)\n",
    "            text = text.replace(\"\\n\\n\", \"\")\n",
    "        sent_sep_pattern = re.compile('([﹒﹔﹖﹗．。！？][\"’”」』]{0,2}|(?=[\"‘“「『]{1,2}|$))')  # del ：；\n",
    "        sent_list = []\n",
    "        for ele in sent_sep_pattern.split(text):\n",
    "            if sent_sep_pattern.match(ele) and sent_list:\n",
    "                sent_list[-1] += ele\n",
    "            elif ele:\n",
    "                sent_list.append(ele)\n",
    "        return sent_list\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:   ##此处需要进一步优化逻辑\n",
    "        if self.pdf:\n",
    "            text = re.sub(r\"\\n{3,}\", r\"\\n\", text)\n",
    "            text = re.sub('\\s', \" \", text)\n",
    "            text = re.sub(\"\\n\\n\", \"\", text)\n",
    "\n",
    "        text = re.sub(r'([;；.!?。！？\\?])([^”’])', r\"\\1\\n\\2\", text)  # 单字符断句符\n",
    "        text = re.sub(r'(\\.{6})([^\"’”」』])', r\"\\1\\n\\2\", text)  # 英文省略号\n",
    "        text = re.sub(r'(\\…{2})([^\"’”」』])', r\"\\1\\n\\2\", text)  # 中文省略号\n",
    "        text = re.sub(r'([;；!?。！？\\?][\"’”」』]{0,2})([^;；!?，。！？\\?])', r'\\1\\n\\2', text)\n",
    "        # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "        text = text.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "        # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "        ls = [i for i in text.split(\"\\n\") if i]\n",
    "        for ele in ls:\n",
    "            if len(ele) > self.sentence_size:\n",
    "                ele1 = re.sub(r'([,，.][\"’”」』]{0,2})([^,，.])', r'\\1\\n\\2', ele)\n",
    "                ele1_ls = ele1.split(\"\\n\")\n",
    "                for ele_ele1 in ele1_ls:\n",
    "                    if len(ele_ele1) > self.sentence_size:\n",
    "                        ele_ele2 = re.sub(r'([\\n]{1,}| {2,}[\"’”」』]{0,2})([^\\s])', r'\\1\\n\\2', ele_ele1)\n",
    "                        ele2_ls = ele_ele2.split(\"\\n\")\n",
    "                        for ele_ele2 in ele2_ls:\n",
    "                            if len(ele_ele2) > self.sentence_size:\n",
    "                                ele_ele3 = re.sub('( [\"’”」』]{0,2})([^ ])', r'\\1\\n\\2', ele_ele2)\n",
    "                                ele2_id = ele2_ls.index(ele_ele2)\n",
    "                                ele2_ls = ele2_ls[:ele2_id] + [i for i in ele_ele3.split(\"\\n\") if i] + ele2_ls[\n",
    "                                                                                                       ele2_id + 1:]\n",
    "                        ele_id = ele1_ls.index(ele_ele1)\n",
    "                        ele1_ls = ele1_ls[:ele_id] + [i for i in ele2_ls if i] + ele1_ls[ele_id + 1:]\n",
    "\n",
    "                id = ls.index(ele)\n",
    "                ls = ls[:id] + [i for i in ele1_ls if i] + ls[id + 1:]\n",
    "        return ls\n",
    "\n",
    "filepath = \"data/faq/ecommerce_faq.txt\"\n",
    "loader = TextLoader(filepath)\n",
    "textsplitter = ChineseTextSplitter(pdf=False, sentence_size=SENTENCE_SIZE)\n",
    "docs = loader.load_and_split(textsplitter)\n",
    "\n",
    "vs_path = \"./vector_storage\"\n",
    "faiss_vector_store = FAISS.from_documents(docs, hgf_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 107.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 支持哪些省份配送？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "faiss_vector_store.save_local(vs_path)\n",
    "\n",
    "faiss_vector_store = FAISS.load_local(vs_path, hgf_embeddings)\n",
    "query_faiss = \"配送范围是？\"\n",
    "result_docs = faiss_vector_store.similarity_search(query=query_faiss)\n",
    "print(result_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChain\nllm\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m      3\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39m基于以下已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，请说 \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m根据已知信息无法回答该问题\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m 或 \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m没有提供足够的相关信息\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m，不允许在答案中添加编造成分，答案请使用中文。\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m        已知内容:\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39m        \u001b[39m\u001b[39m{result_docs}\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m        问题:\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m        \u001b[39m\u001b[39m{query_faiss}\u001b[39;00m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mchain_type：chain类型\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    stuff: 这种最简单粗暴，会把所有的 document 一次全部传给 llm 模型进行总结。如果document很多的话，势必会报超出最大 token 限制的错，所以总结文本的时候一般不会选中这个。\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m qa \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39;49mfrom_chain_type(llm\u001b[39m=\u001b[39;49mmodel, chain_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstuff\u001b[39;49m\u001b[39m\"\u001b[39;49m, retriever\u001b[39m=\u001b[39;49mfaiss_vector_store\u001b[39m.\u001b[39;49mas_retriever())\n\u001b[1;32m     19\u001b[0m qa\u001b[39m.\u001b[39mrun(query_faiss)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/langchain/chains/retrieval_qa/base.py:91\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m _chain_type_kwargs \u001b[39m=\u001b[39m chain_type_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m---> 91\u001b[0m combine_documents_chain \u001b[39m=\u001b[39m load_qa_chain(\n\u001b[1;32m     92\u001b[0m     llm, chain_type\u001b[39m=\u001b[39;49mchain_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_chain_type_kwargs\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(combine_documents_chain\u001b[39m=\u001b[39mcombine_documents_chain, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/langchain/chains/question_answering/__init__.py:218\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m chain_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loader_mapping:\n\u001b[1;32m    214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unsupported chain type: \u001b[39m\u001b[39m{\u001b[39;00mchain_type\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShould be one of \u001b[39m\u001b[39m{\u001b[39;00mloader_mapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m     )\n\u001b[0;32m--> 218\u001b[0m \u001b[39mreturn\u001b[39;00m loader_mapping[chain_type](\n\u001b[1;32m    219\u001b[0m     llm, verbose\u001b[39m=\u001b[39;49mverbose, callback_manager\u001b[39m=\u001b[39;49mcallback_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    220\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/langchain/chains/question_answering/__init__.py:63\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_stuff_chain\u001b[39m(\n\u001b[1;32m     55\u001b[0m     llm: BaseLanguageModel,\n\u001b[1;32m     56\u001b[0m     prompt: Optional[BasePromptTemplate] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     61\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StuffDocumentsChain:\n\u001b[1;32m     62\u001b[0m     _prompt \u001b[39m=\u001b[39m prompt \u001b[39mor\u001b[39;00m stuff_prompt\u001b[39m.\u001b[39mPROMPT_SELECTOR\u001b[39m.\u001b[39mget_prompt(llm)\n\u001b[0;32m---> 63\u001b[0m     llm_chain \u001b[39m=\u001b[39m LLMChain(\n\u001b[1;32m     64\u001b[0m         llm\u001b[39m=\u001b[39;49mllm, prompt\u001b[39m=\u001b[39;49m_prompt, verbose\u001b[39m=\u001b[39;49mverbose, callback_manager\u001b[39m=\u001b[39;49mcallback_manager\n\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m     \u001b[39m# TODO: document prompt\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[39mreturn\u001b[39;00m StuffDocumentsChain(\n\u001b[1;32m     68\u001b[0m         llm_chain\u001b[39m=\u001b[39mllm_chain,\n\u001b[1;32m     69\u001b[0m         document_variable_name\u001b[39m=\u001b[39mdocument_variable_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     73\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for LLMChain\nllm\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "'''\n",
    "chain_type：chain类型\n",
    "    stuff: 这种最简单粗暴，会把所有的 document 一次全部传给 llm 模型进行总结。如果document很多的话，势必会报超出最大 token 限制的错，所以总结文本的时候一般不会选中这个。\n",
    "    map_reduce: 这个方式会先将每个 document 进行总结，最后将所有 document 总结出的结果再进行一次总结。\n",
    "    refine: 这种方式会先总结第一个 document，然后在将第一个 document 总结出的内容和第二个 document 一起发给 llm 模型在进行总结，以此类推。这种方式的好处就是在总结后一个 document 的时候，会带着前一个的 document 进行总结，给需要总结的 document 添加了上下文，增加了总结内容的连贯性。\n",
    "    这种一般不会用在总结的 chain 上，而是会用在问答的 chain 上，他其实是一种搜索答案的匹配方式。首先你要给出一个问题，他会根据问题给每个 document 计算一个这个 document 能回答这个问题的概率分数，然后找到分数最高的那个 document ，在通过把这个 document 转化为问题的 prompt 的一部分（问题+document）发送给 llm 模型，最后 llm 模型返回具体答案。\n",
    "\n",
    "'''\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=model, chain_type=\"stuff\", retriever=faiss_vector_store.as_retriever())\n",
    "qa.run(query_faiss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.11it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m storage_context \u001b[39m=\u001b[39m StorageContext\u001b[39m.\u001b[39mfrom_defaults( docstore\u001b[39m=\u001b[39mSimpleDocumentStore(), vector_store\u001b[39m=\u001b[39mvector_store, index_store\u001b[39m=\u001b[39mSimpleIndexStore())\n\u001b[1;32m     17\u001b[0m storage_context\u001b[39m.\u001b[39mdocstore\u001b[39m.\u001b[39madd_documents(new_nodes)\n\u001b[0;32m---> 18\u001b[0m index_by_faiss \u001b[39m=\u001b[39m GPTVectorStoreIndex(new_nodes, storage_context\u001b[39m=\u001b[39;49mstorage_context, service_context\u001b[39m=\u001b[39;49mservice_context)\n\u001b[1;32m     20\u001b[0m index_by_faiss\u001b[39m.\u001b[39mstorage_context\u001b[39m.\u001b[39mpersist()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:40\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, storage_context, use_async, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async \u001b[39m=\u001b[39m use_async\n\u001b[0;32m---> 40\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     41\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m     42\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[1;32m     43\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m     44\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[1;32m     45\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     46\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/indices/base.py:65\u001b[0m, in \u001b[0;36mBaseGPTIndex.__init__\u001b[0;34m(self, nodes, index_struct, storage_context, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m index_struct \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[39massert\u001b[39;00m nodes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     index_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_index_from_nodes(nodes)\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct \u001b[39m=\u001b[39m index_struct\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context\u001b[39m.\u001b[39mindex_store\u001b[39m.\u001b[39madd_index_struct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/token_counter/token_counter.py:78\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_llm_predict\u001b[39m(_self: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     77\u001b[0m     \u001b[39mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[0;32m---> 78\u001b[0m         f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m f_return_val\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:190\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m@llm_token_counter\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbuild_index_from_nodes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_index_from_nodes\u001b[39m(\u001b[39mself\u001b[39m, nodes: Sequence[Node]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m IndexDict:\n\u001b[1;32m    184\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build the index from nodes.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[39m    NOTE: Overrides BaseGPTIndex.build_index_from_nodes.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m        GPTVectorStoreIndex only stores nodes in document store\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m        if vector store does not store text\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_index_from_nodes(nodes)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:179\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    177\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_nodes_to_index(index_struct, nodes)\n\u001b[1;32m    180\u001b[0m \u001b[39mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:156\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    155\u001b[0m embedding_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_node_embedding_results(nodes)\n\u001b[0;32m--> 156\u001b[0m new_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vector_store\u001b[39m.\u001b[39;49madd(embedding_results)\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39mstores_text:\n\u001b[1;32m    159\u001b[0m     \u001b[39m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39m# we need to add the nodes to the index struct and document store\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39mfor\u001b[39;00m result, new_id \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(embedding_results, new_ids):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/vector_stores/faiss.py:96\u001b[0m, in \u001b[0;36mFaissVectorStore.add\u001b[0;34m(self, embedding_results)\u001b[0m\n\u001b[1;32m     94\u001b[0m     text_embedding_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(text_embedding, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)[np\u001b[39m.\u001b[39mnewaxis, :]\n\u001b[1;32m     95\u001b[0m     new_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_faiss_index\u001b[39m.\u001b[39mntotal)\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_faiss_index\u001b[39m.\u001b[39;49madd(text_embedding_np)\n\u001b[1;32m     97\u001b[0m     new_ids\u001b[39m.\u001b[39mappend(new_id)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m new_ids\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/faiss/__init__.py:214\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_add\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Adds vectors to the index.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mThe index must be trained before vectors can be added to it.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mThe vectors are implicitly numbered in sequence. When `n` vectors are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m    `dtype` must be float32.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m n, d \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 214\u001b[0m \u001b[39massert\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_c(n, swig_ptr(x))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "new_text_splitter = SpacyTextSplitter(pipeline=\"zh_core_web_sm\", chunk_size = 2048)\n",
    "new_parser = SimpleNodeParser(text_splitter=new_text_splitter)\n",
    "new_documents = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "new_nodes = new_parser.get_nodes_from_documents(new_documents)\n",
    "\n",
    "dimention_faiss = 1536\n",
    "faiss_index = faiss.IndexFlatL2(dimention_faiss)\n",
    "\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults( docstore=SimpleDocumentStore(), vector_store=vector_store, index_store=SimpleIndexStore())\n",
    "storage_context.docstore.add_documents(new_nodes)\n",
    "index_by_faiss = GPTVectorStoreIndex(new_nodes, storage_context=storage_context, service_context=service_context)\n",
    "\n",
    "index_by_faiss.storage_context.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading llama_index.vector_stores.faiss from ./storage/vector_store.json.\n",
      "Loading llama_index.vector_stores.faiss from ./storage/vector_store.json.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 57 tokens\n",
      "> [retrieve] Total embedding token usage: 57 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2220 tokens\n",
      "> [get_response] Total LLM token usage: 2220 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "鲁迅先生回忆了他在仙台的经历，他曾经被藤野先生教导，但最终决定不学医学，而是去学生物学。藤野先生有些悲哀，但他叮嘱鲁迅先生将来照相并及时通信告诉他此后的状况。然而，鲁迅先生多年没有照相，也没有写信，从藤野先生的角度看，他已经杳无消息。最后\n"
     ]
    }
   ],
   "source": [
    "# load index from disk\n",
    "vector_store_loaded = FaissVectorStore.from_persist_dir('./storage')\n",
    "# 下面这个部分必须要提供persist_dir这个路径，似乎缺省不起作用\n",
    "storage_context_loaded = StorageContext.from_defaults(vector_store=vector_store_loaded, persist_dir=\"./storage\")\n",
    "index_by_faiss_loaded = load_index_from_storage(storage_context=storage_context_loaded)\n",
    "\n",
    "query_engine = index_by_faiss_loaded.as_query_engine()\n",
    "response = query_engine.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt template的支持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 7055 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 7055 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 217.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2122 tokens\n",
      "> [get_response] Total LLM token usage: 2122 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "根据上下文信息，可以看出作者在谈论自己的旅行经历和在日本的经历。在藤野先生的段落中，作者提到了东京的樱花烂熳，但同时也提到了留学生的速成班。因此，可以推断藤野先生所在的段落讨论的并非樱花，而是留学生。此外，在藤野先生的段落中，作者提到了仙台医学专门学校，并在此讲述了自己的旅行经历。因此，可以得出结论，作者并没有提到海南或任何与海南相关的事物，因此，回答是：不能。\n"
     ]
    }
   ],
   "source": [
    "from llama_index import QuestionAnswerPrompt\n",
    "\n",
    "# load documents\n",
    "new_documents = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "\n",
    "# define custom QuestionAnswerPrompt\n",
    "query_str = \"请问你们海南能发货吗？\"\n",
    "QA_PROMPT_TMPL = (\n",
    "    \"请参考如下上下文信息. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"基于这样的上下文信息，请回答: {query_str}\\n\"\n",
    ")\n",
    "QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
    "# Build GPTVectorStoreIndex\n",
    "index = GPTVectorStoreIndex.from_documents(new_documents, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    text_qa_template=QA_PROMPT\n",
    ")\n",
    "response = query_engine.query(query_str)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
