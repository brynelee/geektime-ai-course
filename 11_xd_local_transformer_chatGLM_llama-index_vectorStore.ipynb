{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全部采用本地化部署的方式和开源模型来建立的问答能力和本地化知识库集成的能力"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaodong/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.c -shared -o /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.so\n",
      "Load kernel : /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 6\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照langchain标准进行模型封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from llama_index import LLMPredictor\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response, history = model.chat(tokenizer, prompt, history=[])\n",
    "        # only return newly generated tokens\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"name_of_model\": \"THUDM/chatglm-6b-int4\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入embedding模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义包含embedding模型和语言模型的Service Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建本地知识库（本地知识的索引库）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=80, chunk_overlap=20)\n",
    "\n",
    "documents = SimpleDirectoryReader('./data/faq/').load_data()\n",
    "\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents=documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合本地知识库和语言模型进行提问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1878 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 我们的物流公司可以配送到天津。关于退货政策，我们在订单发货后的24小时内提供在线客服支持，方便用户联系处理退货事宜。具体的退货政策请参考订单页面或联系客服咨询。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 你们能配送到天津吗？另外，你们的退货政策是怎样的？\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLM-6B 是由清华大学 KEG 实验室和智谱 AI 公司于 2023年 共同开发的人工智能助手。它是基于 GLM-6B 模型开发的，能够针对用户的问题和要求提供适当的答复和支持。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 请介绍一下ChatGLM？\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca是一个开源的语言模型，由清华大学 KEG 实验室和智谱AI开发。它是基于 GLM-130B 模型开发的，拥有 1300 亿个参数，是GLM-130B 的 10 倍。Alpaca 可以帮助人类完成一些语言任务，如文本生成、翻译、问答等。它可以通过自然语言处理技术，对文本进行建模，并生成与输入文本相似的自然语言输出。Alpaca 可以在各种平台上使用，包括 Python、TensorFlow、PyTorch 等。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 请介绍一下语言模型中的Alpaca。\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是一段Python代码，展示如何使用Alpaca的模型在本地GPU运行支持语言任务的能力：\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "\n",
      "# 加载模型\n",
      "model = tf.keras.models.load_model('model.json')\n",
      "\n",
      "# 定义训练函数\n",
      "def train_model(x_train, y_train, batch_size, epochs):\n",
      "    # 定义损失函数\n",
      "    def loss_fn(x, y):\n",
      "        return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_train > 0)\n",
      "\n",
      "    # 定义优化函数\n",
      "    def optimize_model(model, x_train, y_train, batch_size, epochs):\n",
      "        # 计算损失函数\n",
      "        loss = loss_fn(model.predict(x_train), y_train)\n",
      "\n",
      "        # 计算梯度\n",
      "        梯度 = model.trainable_variables[0] * loss\n",
      "\n",
      "        # 更新模型参数\n",
      "        model.trainable_variables[0] +=梯度\n",
      "\n",
      "        # 计算反向传播\n",
      "        反向传播 = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_train, logits=model.predict(x_train)))\n",
      "\n",
      "        # 计算优化目标\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
      "\n",
      "        # 训练模型\n",
      "        model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
      "        model.fit(x_train, y_train, epochs=epochs)\n",
      "\n",
      "# 运行训练函数\n",
      "train_model(x_train, y_train, batch_size=10, epochs=10)\n",
      "```\n",
      "\n",
      "这段代码使用TensorFlow的Keras模型来训练一个支持语言任务的模型。训练函数定义了损失函数和优化函数，并使用Adam优化器来训练模型。在训练过程中，模型将输入x_train和y_train映射到输出y_train上，并计算损失函数。然后，优化器将损失函数计算到梯度上，并更新模型参数。最后，模型将训练过程中的损失函数计算到梯度上，并更新模型参数，以使损失函数最小化。在运行训练函数之前，需要加载已经预训练的Alpaca模型，代码中使用了`tf.keras.models.load_model('model.json')`函数来加载模型。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 请写一段python代码，展示如何使用Alpaca的模型在本地GPU运行支持语言任务的能力。\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
