{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全部采用本地化部署的方式和开源模型来建立的问答能力和本地化知识库集成的能力"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaodong/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.c -shared -o /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.so\n",
      "Load kernel : /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 6\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照langchain标准进行模型封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from llama_index import LLMPredictor\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response, history = model.chat(tokenizer, prompt, history=[])\n",
    "        # only return newly generated tokens\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"name_of_model\": \"THUDM/chatglm-6b-int4\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入embedding模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义包含embedding模型和语言模型的Service Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建本地知识库（本地知识的索引库）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 130, which is longer than the specified 100\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import GPTVectorStoreIndex\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=100, chunk_overlap=20)\n",
    "parser = SimpleNodeParser(text_splitter=text_splitter)\n",
    "documents = SimpleDirectoryReader('./data/faq/').load_data()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "\n",
    "index = GPTVectorStoreIndex(nodes=nodes, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合本地知识库和语言模型进行提问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 我们的配送范围是全国大部分省份，包括北京、上海、天津、重庆、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆。因此，我们在天津的配送是可行的。至于退货政策，我们会在订单发货后尽快处理退货，并在“我的订单”页面展示退货说明，请按照说明进行退货。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 你们能配送到天津吗？另外，你们的退货政策是怎样的？\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLM-6B 是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型 GLM-6B 开发的，我的任务是针对用户的问题和要求提供适当的答复和支持。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 请介绍一下ChatGLM？\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Alpaca是一个由清华大学 KEG 实验室和智谱AI开发的语言模型，旨在提供自然语言处理和生成的能力。它使用深度学习技术来训练，并能够生成高质量的文本，例如文章、对话和邮件。Alpaca 还可以进行语言理解和文本生成，因此可以与人类进行对话，并生成各种类型的文本。它的主要优势在于其质量和速度，可以处理大量的文本数据，并且可以自动学习和改进。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 请介绍一下语言模型中的Alpaca。\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是一段使用Python代码展示如何使用Alpaca模型在本地GPU运行支持语言任务的能力的步骤：\n",
      "\n",
      "1. 安装必要的库\n",
      "\n",
      "首先，需要安装必要的库，包括PyTorch和CUDA。可以使用以下命令安装：\n",
      "\n",
      "```\n",
      "pip install torch torchvision\n",
      "pip install CUDA\n",
      "```\n",
      "\n",
      "2. 导入必要的库\n",
      "\n",
      "导入必要的库，包括PyTorch和CUDA库。可以使用以下代码导入：\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torchvision.models as models\n",
      "import CUDART\n",
      "```\n",
      "\n",
      "3. 加载Alpaca模型\n",
      "\n",
      "可以使用以下代码加载Alpaca模型：\n",
      "\n",
      "```\n",
      "model = CUDART.load('your_paca_model_path', 'cuda')\n",
      "```\n",
      "\n",
      "4. 运行GPU训练\n",
      "\n",
      "使用以下代码运行GPU训练：\n",
      "\n",
      "```\n",
      "device = CUDART.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "model.to(device)\n",
      "```\n",
      "\n",
      "5. 运行本地GPU训练\n",
      "\n",
      "可以使用以下代码运行本地GPU训练：\n",
      "\n",
      "```\n",
      "device = CUDART.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "model.to(device)\n",
      "model.eval()\n",
      "```\n",
      "\n",
      "以上代码展示了如何使用PyTorch和CUDA库来加载Alpaca模型并在本地GPU上运行训练。\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "请用中文回答下述问题：\n",
    "\n",
    "Q: 请写一段python代码，展示如何使用Alpaca的模型在本地GPU运行支持语言任务的能力。\n",
    "\"\"\"\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用新的TextSplitter来对原始内容进行处理，换一个新的index来处理索引，对文章进行总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "new_text_splitter = SpacyTextSplitter(pipeline=\"zh_core_web_sm\", chunk_size = 2048)\n",
    "new_parser = SimpleNodeParser(text_splitter=new_text_splitter)\n",
    "new_documents = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "new_nodes = new_parser.get_nodes_from_documents(new_documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1680 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这段内容主要是描述鲁迅先生在日本留学期间的经历和对于中国社会、文化、政治等方面的观察和思考。其中包括他在日本仙台医学专门学校学习的经历，与学校的老师们和学生们之间的互动和交往，以及他对中国传统文化和医学知识的热爱和追求。同时，他表达了对藤野先生的感激之情，同时也对他穿衣搭配和学术成就等方面进行了赞扬和评价。\n",
      "\n",
      "整段内容表达了鲁迅先生对这位老师的感激之情和敬意，也展现了他对于学术和知识的追求和热爱。他描述了他收藏的讲义，以及他对于夜间思考和良心发现的经历。这段内容主要讲述了他在日本留学期间的经历和对于中国社会、文化、政治等方面的观察和思考，是《藤野先生》节选的全文总结。\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTListIndex\n",
    "\n",
    "list_index = GPTListIndex(nodes=new_nodes, service_context=service_context)\n",
    "\n",
    "# response = list_index.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\", response_mode=\"tree_summarize\")\n",
    "\n",
    "query_engine = list_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "response = query_engine.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换用TreeIndex来试试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是新的总结：\n",
      "\n",
      "在这段文字中，鲁迅先生描述了自己在日本医学专门学校学习的经历，以及与藤野先生相处的时光。\n",
      "\n",
      "自己最初来到仙台，被学校收为学生，住在监狱旁边的客店里，蚊子太多，饭食不好，但好处是学校不要学费，还提供住宿和饮食。后来，学生潮中有中国留学生，于是去研究他们的文化和生活方式。\n",
      "\n",
      "在学生中，有一位藤野先生，他的衣服很时髦，但会忘记带领结，有时也会和学生们一起跳舞。他讲述了医学解剖的历史，强调了中国的医学翻译和研究，也提到了他的学生中的一些有趣人物。\n",
      "\n",
      "最后，藤野先生将自己的讲义修改过，并且每星期都要送自己看一次，让自己对医学知识有了更深入的了解。\n",
      "\n",
      "在学年试验之后，鲁迅先生去了东京玩一夏天，秋初再回学校，成绩早已发表了，同学一百余人之中，自己在中间，不过是没有落第。这回藤野先生所担任的功课，是解剖实习和局部解剖学。\n",
      "\n",
      "解剖实习了大概一星期，藤野先生又叫我去了，很高兴地，仍用了极有抑扬的声调对我说道：——\n",
      "“我因为听说中国人是很敬重鬼的，所以很担心，怕你不肯解剖尸体。\n",
      "\n",
      "现在总算放心了，没有这回事。”\n",
      "\n",
      "但他也偶有使我很为难的时候。\n",
      "\n",
      "他听说中国的女人是裹脚的，但不知道详细，所以要问我怎么裹法，足骨变成怎样的畸形，还叹息道，“总要看一看才知道。\n",
      "\n",
      "究竟是怎么一回事呢？”\n",
      "\n",
      "有一天，本级的学生会干事到我寓里来了，要借我的讲义看。\n",
      "\n",
      "我检出来交给他们，却只翻检了一通，并没有带走。\n",
      "\n",
      "但他们一走，邮差就送到一封很厚的信，打开一看，第一句是：——\n",
      "“你改悔罢！”\n",
      "这是《新约》上的句子罢，但经托尔斯泰新近引用过的。\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTTreeIndex\n",
    "\n",
    "# define LLM\n",
    "tree_index = GPTTreeIndex(nodes=new_nodes, service_context=service_context)\n",
    "# response = tree_index.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\", mode=\"summarize\")\n",
    "\n",
    "#query_engine = tree_index.as_query_engine(\n",
    "#    response_mode=\"summarize\"\n",
    "#)\n",
    "\n",
    "query_engine = tree_index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试Vector Store - pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 目前这个不分不工作\\nimport pinecone\\nfrom llama_index import StorageContext\\nfrom llama_index.vector_stores import PineconeVectorStore\\nimport os\\n\\n# pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\\n# pinecone_env = os.environ.get(\"PINECONE_ENV\")\\n\\npinecone_api_key = os.environ[\\'PINECONE_API_KEY\\']\\npinecone_env = os.environ[\\'PINECONE_ENV\\']\\n\\n\\n# init pinecone\\npinecone.init(pinecone_api_key, pinecone_env)\\npinecone.create_index(\"xdtest1\", dimension=1536, metric=\"cosine\", pod_type=\"p1\") # pinecone.create_index(\"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")\\n\\n# construct vector store and customize storage context\\nstorage_context = StorageContext.from_defaults(\\n    vector_store = PineconeVectorStore(pinecone.Index(\"xdtest1\"))\\n)\\n\\n# Load documents and build index\\ndocuments_p = SimpleDirectoryReader(\\'./data/mr_fujino\\').load_data()\\nindex_p = GPTVectorStoreIndex.from_documents(documents_p, service_context=service_context, storage_context=storage_context)\\n\\nquery_engine = index_p.as_query_engine()\\n\\nresponse = query_engine.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\")\\n\\nprint(response)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 目前这个不分不工作\n",
    "import pinecone\n",
    "from llama_index import StorageContext\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "import os\n",
    "\n",
    "# pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "# pinecone_env = os.environ.get(\"PINECONE_ENV\")\n",
    "\n",
    "pinecone_api_key = os.environ['PINECONE_API_KEY']\n",
    "pinecone_env = os.environ['PINECONE_ENV']\n",
    "\n",
    "\n",
    "# init pinecone\n",
    "pinecone.init(pinecone_api_key, pinecone_env)\n",
    "pinecone.create_index(\"xdtest1\", dimension=1536, metric=\"cosine\", pod_type=\"p1\") # pinecone.create_index(\"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")\n",
    "\n",
    "# construct vector store and customize storage context\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store = PineconeVectorStore(pinecone.Index(\"xdtest1\"))\n",
    ")\n",
    "\n",
    "# Load documents and build index\n",
    "documents_p = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "index_p = GPTVectorStoreIndex.from_documents(documents_p, service_context=service_context, storage_context=storage_context)\n",
    "\n",
    "query_engine = index_p.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\")\n",
    "\n",
    "print(response)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss Vector Store的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: GanymedeNil/text2vec-large-chinese\n",
      "Load pretrained SentenceTransformer: GanymedeNil/text2vec-large-chinese\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /home/xiaodong/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name /home/xiaodong/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  6.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import UnstructuredFileLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "em_model = \"GanymedeNil/text2vec-large-chinese\"\n",
    "em_device = \"cuda\"\n",
    "\n",
    "hgf_embeddings = HuggingFaceEmbeddings(model_name=em_model,\n",
    "                                        model_kwargs={'device': em_device})\n",
    "\n",
    "import re\n",
    "\n",
    "# 文本分句长度\n",
    "SENTENCE_SIZE = 100\n",
    "\n",
    "class ChineseTextSplitter(CharacterTextSplitter):\n",
    "    def __init__(self, pdf: bool = False, sentence_size: int = SENTENCE_SIZE, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pdf = pdf\n",
    "        self.sentence_size = sentence_size\n",
    "\n",
    "    def split_text1(self, text: str) -> List[str]:\n",
    "        if self.pdf:\n",
    "            text = re.sub(r\"\\n{3,}\", \"\\n\", text)\n",
    "            text = re.sub('\\s', ' ', text)\n",
    "            text = text.replace(\"\\n\\n\", \"\")\n",
    "        sent_sep_pattern = re.compile('([﹒﹔﹖﹗．。！？][\"’”」』]{0,2}|(?=[\"‘“「『]{1,2}|$))')  # del ：；\n",
    "        sent_list = []\n",
    "        for ele in sent_sep_pattern.split(text):\n",
    "            if sent_sep_pattern.match(ele) and sent_list:\n",
    "                sent_list[-1] += ele\n",
    "            elif ele:\n",
    "                sent_list.append(ele)\n",
    "        return sent_list\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:   ##此处需要进一步优化逻辑\n",
    "        if self.pdf:\n",
    "            text = re.sub(r\"\\n{3,}\", r\"\\n\", text)\n",
    "            text = re.sub('\\s', \" \", text)\n",
    "            text = re.sub(\"\\n\\n\", \"\", text)\n",
    "\n",
    "        text = re.sub(r'([;；.!?。！？\\?])([^”’])', r\"\\1\\n\\2\", text)  # 单字符断句符\n",
    "        text = re.sub(r'(\\.{6})([^\"’”」』])', r\"\\1\\n\\2\", text)  # 英文省略号\n",
    "        text = re.sub(r'(\\…{2})([^\"’”」』])', r\"\\1\\n\\2\", text)  # 中文省略号\n",
    "        text = re.sub(r'([;；!?。！？\\?][\"’”」』]{0,2})([^;；!?，。！？\\?])', r'\\1\\n\\2', text)\n",
    "        # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "        text = text.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "        # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "        ls = [i for i in text.split(\"\\n\") if i]\n",
    "        for ele in ls:\n",
    "            if len(ele) > self.sentence_size:\n",
    "                ele1 = re.sub(r'([,，.][\"’”」』]{0,2})([^,，.])', r'\\1\\n\\2', ele)\n",
    "                ele1_ls = ele1.split(\"\\n\")\n",
    "                for ele_ele1 in ele1_ls:\n",
    "                    if len(ele_ele1) > self.sentence_size:\n",
    "                        ele_ele2 = re.sub(r'([\\n]{1,}| {2,}[\"’”」』]{0,2})([^\\s])', r'\\1\\n\\2', ele_ele1)\n",
    "                        ele2_ls = ele_ele2.split(\"\\n\")\n",
    "                        for ele_ele2 in ele2_ls:\n",
    "                            if len(ele_ele2) > self.sentence_size:\n",
    "                                ele_ele3 = re.sub('( [\"’”」』]{0,2})([^ ])', r'\\1\\n\\2', ele_ele2)\n",
    "                                ele2_id = ele2_ls.index(ele_ele2)\n",
    "                                ele2_ls = ele2_ls[:ele2_id] + [i for i in ele_ele3.split(\"\\n\") if i] + ele2_ls[\n",
    "                                                                                                       ele2_id + 1:]\n",
    "                        ele_id = ele1_ls.index(ele_ele1)\n",
    "                        ele1_ls = ele1_ls[:ele_id] + [i for i in ele2_ls if i] + ele1_ls[ele_id + 1:]\n",
    "\n",
    "                id = ls.index(ele)\n",
    "                ls = ls[:id] + [i for i in ele1_ls if i] + ls[id + 1:]\n",
    "        return ls\n",
    "\n",
    "filepath = \"data/faq/ecommerce_faq.txt\"\n",
    "loader = TextLoader(filepath)\n",
    "textsplitter = ChineseTextSplitter(pdf=False, sentence_size=SENTENCE_SIZE)\n",
    "docs = loader.load_and_split(textsplitter)\n",
    "\n",
    "vs_path = \"./vector_storage\"\n",
    "faiss_vector_store = FAISS.from_documents(docs, hgf_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 104.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 支持哪些省份配送？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "faiss_vector_store.save_local(vs_path)\n",
    "\n",
    "faiss_vector_store = FAISS.load_local(vs_path, hgf_embeddings)\n",
    "query_faiss = \"配送范围是？\"\n",
    "result_docs = faiss_vector_store.similarity_search(query=query_faiss)\n",
    "print(result_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.11it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m storage_context \u001b[39m=\u001b[39m StorageContext\u001b[39m.\u001b[39mfrom_defaults( docstore\u001b[39m=\u001b[39mSimpleDocumentStore(), vector_store\u001b[39m=\u001b[39mvector_store, index_store\u001b[39m=\u001b[39mSimpleIndexStore())\n\u001b[1;32m     17\u001b[0m storage_context\u001b[39m.\u001b[39mdocstore\u001b[39m.\u001b[39madd_documents(new_nodes)\n\u001b[0;32m---> 18\u001b[0m index_by_faiss \u001b[39m=\u001b[39m GPTVectorStoreIndex(new_nodes, storage_context\u001b[39m=\u001b[39;49mstorage_context, service_context\u001b[39m=\u001b[39;49mservice_context)\n\u001b[1;32m     20\u001b[0m index_by_faiss\u001b[39m.\u001b[39mstorage_context\u001b[39m.\u001b[39mpersist()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:40\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, storage_context, use_async, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async \u001b[39m=\u001b[39m use_async\n\u001b[0;32m---> 40\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     41\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m     42\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[1;32m     43\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m     44\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[1;32m     45\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     46\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/indices/base.py:65\u001b[0m, in \u001b[0;36mBaseGPTIndex.__init__\u001b[0;34m(self, nodes, index_struct, storage_context, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m index_struct \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[39massert\u001b[39;00m nodes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     index_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_index_from_nodes(nodes)\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct \u001b[39m=\u001b[39m index_struct\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context\u001b[39m.\u001b[39mindex_store\u001b[39m.\u001b[39madd_index_struct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/token_counter/token_counter.py:78\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_llm_predict\u001b[39m(_self: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     77\u001b[0m     \u001b[39mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[0;32m---> 78\u001b[0m         f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m f_return_val\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:190\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m@llm_token_counter\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbuild_index_from_nodes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_index_from_nodes\u001b[39m(\u001b[39mself\u001b[39m, nodes: Sequence[Node]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m IndexDict:\n\u001b[1;32m    184\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build the index from nodes.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[39m    NOTE: Overrides BaseGPTIndex.build_index_from_nodes.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m        GPTVectorStoreIndex only stores nodes in document store\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m        if vector store does not store text\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_index_from_nodes(nodes)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:179\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    177\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_nodes_to_index(index_struct, nodes)\n\u001b[1;32m    180\u001b[0m \u001b[39mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:156\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    155\u001b[0m embedding_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_node_embedding_results(nodes)\n\u001b[0;32m--> 156\u001b[0m new_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vector_store\u001b[39m.\u001b[39;49madd(embedding_results)\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39mstores_text:\n\u001b[1;32m    159\u001b[0m     \u001b[39m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39m# we need to add the nodes to the index struct and document store\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39mfor\u001b[39;00m result, new_id \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(embedding_results, new_ids):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/llama_index/vector_stores/faiss.py:96\u001b[0m, in \u001b[0;36mFaissVectorStore.add\u001b[0;34m(self, embedding_results)\u001b[0m\n\u001b[1;32m     94\u001b[0m     text_embedding_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(text_embedding, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)[np\u001b[39m.\u001b[39mnewaxis, :]\n\u001b[1;32m     95\u001b[0m     new_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_faiss_index\u001b[39m.\u001b[39mntotal)\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_faiss_index\u001b[39m.\u001b[39;49madd(text_embedding_np)\n\u001b[1;32m     97\u001b[0m     new_ids\u001b[39m.\u001b[39mappend(new_id)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m new_ids\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/faiss/__init__.py:214\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_add\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Adds vectors to the index.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mThe index must be trained before vectors can be added to it.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mThe vectors are implicitly numbered in sequence. When `n` vectors are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m    `dtype` must be float32.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m n, d \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 214\u001b[0m \u001b[39massert\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_c(n, swig_ptr(x))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "new_text_splitter = SpacyTextSplitter(pipeline=\"zh_core_web_sm\", chunk_size = 2048)\n",
    "new_parser = SimpleNodeParser(text_splitter=new_text_splitter)\n",
    "new_documents = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "new_nodes = new_parser.get_nodes_from_documents(new_documents)\n",
    "\n",
    "dimention_faiss = 1536\n",
    "faiss_index = faiss.IndexFlatL2(dimention_faiss)\n",
    "\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults( docstore=SimpleDocumentStore(), vector_store=vector_store, index_store=SimpleIndexStore())\n",
    "storage_context.docstore.add_documents(new_nodes)\n",
    "index_by_faiss = GPTVectorStoreIndex(new_nodes, storage_context=storage_context, service_context=service_context)\n",
    "\n",
    "index_by_faiss.storage_context.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading llama_index.vector_stores.faiss from ./storage/vector_store.json.\n",
      "Loading llama_index.vector_stores.faiss from ./storage/vector_store.json.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 57 tokens\n",
      "> [retrieve] Total embedding token usage: 57 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2220 tokens\n",
      "> [get_response] Total LLM token usage: 2220 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "鲁迅先生回忆了他在仙台的经历，他曾经被藤野先生教导，但最终决定不学医学，而是去学生物学。藤野先生有些悲哀，但他叮嘱鲁迅先生将来照相并及时通信告诉他此后的状况。然而，鲁迅先生多年没有照相，也没有写信，从藤野先生的角度看，他已经杳无消息。最后\n"
     ]
    }
   ],
   "source": [
    "# load index from disk\n",
    "vector_store_loaded = FaissVectorStore.from_persist_dir('./storage')\n",
    "# 下面这个部分必须要提供persist_dir这个路径，似乎缺省不起作用\n",
    "storage_context_loaded = StorageContext.from_defaults(vector_store=vector_store_loaded, persist_dir=\"./storage\")\n",
    "index_by_faiss_loaded = load_index_from_storage(storage_context=storage_context_loaded)\n",
    "\n",
    "query_engine = index_by_faiss_loaded.as_query_engine()\n",
    "response = query_engine.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt template的支持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 7055 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 7055 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 217.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 24 tokens\n",
      "> [retrieve] Total embedding token usage: 24 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2122 tokens\n",
      "> [get_response] Total LLM token usage: 2122 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "根据上下文信息，可以看出作者在谈论自己的旅行经历和在日本的经历。在藤野先生的段落中，作者提到了东京的樱花烂熳，但同时也提到了留学生的速成班。因此，可以推断藤野先生所在的段落讨论的并非樱花，而是留学生。此外，在藤野先生的段落中，作者提到了仙台医学专门学校，并在此讲述了自己的旅行经历。因此，可以得出结论，作者并没有提到海南或任何与海南相关的事物，因此，回答是：不能。\n"
     ]
    }
   ],
   "source": [
    "from llama_index import QuestionAnswerPrompt\n",
    "\n",
    "# load documents\n",
    "new_documents = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "\n",
    "# define custom QuestionAnswerPrompt\n",
    "query_str = \"请问你们海南能发货吗？\"\n",
    "QA_PROMPT_TMPL = (\n",
    "    \"请参考如下上下文信息. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"基于这样的上下文信息，请回答: {query_str}\\n\"\n",
    ")\n",
    "QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
    "# Build GPTVectorStoreIndex\n",
    "index = GPTVectorStoreIndex.from_documents(new_documents, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    text_qa_template=QA_PROMPT\n",
    ")\n",
    "response = query_engine.query(query_str)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
