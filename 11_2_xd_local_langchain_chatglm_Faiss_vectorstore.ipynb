{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss Vector Store的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def torch_gc():\n",
    "    if torch.cuda.is_available():\n",
    "        # with torch.cuda.device(DEVICE):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        try:\n",
    "            from torch.mps import empty_cache\n",
    "            empty_cache()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"如果您使用的是 macOS 建议将 pytorch 版本升级至 2.0.0 或更高版本，以支持及时清理 torch 产生的内存占用。\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaodong/anaconda3/envs/chatglm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.c -shared -o /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.so\n",
      "Load kernel : /home/xiaodong/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 6\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define CustomerLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from llama_index import LLMPredictor\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response, history = model.chat(tokenizer, prompt, history=[])\n",
    "        # only return newly generated tokens\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"name_of_model\": \"THUDM/chatglm-6b-int4\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another LLM dolly 2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: GanymedeNil/text2vec-large-chinese\n",
      "Load pretrained SentenceTransformer: GanymedeNil/text2vec-large-chinese\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /home/xiaodong/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name /home/xiaodong/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "em_model = \"GanymedeNil/text2vec-large-chinese\"\n",
    "em_device = \"cuda\"\n",
    "\n",
    "hgf_embeddings = HuggingFaceEmbeddings(model_name=em_model,\n",
    "                                        model_kwargs={'device': em_device})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "Successfully loaded faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import UnstructuredFileLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import re\n",
    "\n",
    "# 文本分句长度\n",
    "SENTENCE_SIZE = 100\n",
    "\n",
    "class ChineseTextSplitter(CharacterTextSplitter):\n",
    "    def __init__(self, pdf: bool = False, sentence_size: int = SENTENCE_SIZE, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pdf = pdf\n",
    "        self.sentence_size = sentence_size\n",
    "\n",
    "    def split_text1(self, text: str) -> List[str]:\n",
    "        if self.pdf:\n",
    "            text = re.sub(r\"\\n{3,}\", \"\\n\", text)\n",
    "            text = re.sub('\\s', ' ', text)\n",
    "            text = text.replace(\"\\n\\n\", \"\")\n",
    "        sent_sep_pattern = re.compile('([﹒﹔﹖﹗．。！？][\"’”」』]{0,2}|(?=[\"‘“「『]{1,2}|$))')  # del ：；\n",
    "        sent_list = []\n",
    "        for ele in sent_sep_pattern.split(text):\n",
    "            if sent_sep_pattern.match(ele) and sent_list:\n",
    "                sent_list[-1] += ele\n",
    "            elif ele:\n",
    "                sent_list.append(ele)\n",
    "        return sent_list\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:   ##此处需要进一步优化逻辑\n",
    "        if self.pdf:\n",
    "            text = re.sub(r\"\\n{3,}\", r\"\\n\", text)\n",
    "            text = re.sub('\\s', \" \", text)\n",
    "            text = re.sub(\"\\n\\n\", \"\", text)\n",
    "\n",
    "        text = re.sub(r'([;；.!?。！？\\?])([^”’])', r\"\\1\\n\\2\", text)  # 单字符断句符\n",
    "        text = re.sub(r'(\\.{6})([^\"’”」』])', r\"\\1\\n\\2\", text)  # 英文省略号\n",
    "        text = re.sub(r'(\\…{2})([^\"’”」』])', r\"\\1\\n\\2\", text)  # 中文省略号\n",
    "        text = re.sub(r'([;；!?。！？\\?][\"’”」』]{0,2})([^;；!?，。！？\\?])', r'\\1\\n\\2', text)\n",
    "        # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "        text = text.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "        # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "        ls = [i for i in text.split(\"\\n\") if i]\n",
    "        for ele in ls:\n",
    "            if len(ele) > self.sentence_size:\n",
    "                ele1 = re.sub(r'([,，.][\"’”」』]{0,2})([^,，.])', r'\\1\\n\\2', ele)\n",
    "                ele1_ls = ele1.split(\"\\n\")\n",
    "                for ele_ele1 in ele1_ls:\n",
    "                    if len(ele_ele1) > self.sentence_size:\n",
    "                        ele_ele2 = re.sub(r'([\\n]{1,}| {2,}[\"’”」』]{0,2})([^\\s])', r'\\1\\n\\2', ele_ele1)\n",
    "                        ele2_ls = ele_ele2.split(\"\\n\")\n",
    "                        for ele_ele2 in ele2_ls:\n",
    "                            if len(ele_ele2) > self.sentence_size:\n",
    "                                ele_ele3 = re.sub('( [\"’”」』]{0,2})([^ ])', r'\\1\\n\\2', ele_ele2)\n",
    "                                ele2_id = ele2_ls.index(ele_ele2)\n",
    "                                ele2_ls = ele2_ls[:ele2_id] + [i for i in ele_ele3.split(\"\\n\") if i] + ele2_ls[\n",
    "                                                                                                       ele2_id + 1:]\n",
    "                        ele_id = ele1_ls.index(ele_ele1)\n",
    "                        ele1_ls = ele1_ls[:ele_id] + [i for i in ele2_ls if i] + ele1_ls[ele_id + 1:]\n",
    "\n",
    "                id = ls.index(ele)\n",
    "                ls = ls[:id] + [i for i in ele1_ls if i] + ls[id + 1:]\n",
    "        return ls\n",
    "\n",
    "filepath = \"data/faq/ecommerce_faq.txt\"\n",
    "loader = TextLoader(filepath)\n",
    "textsplitter = ChineseTextSplitter(pdf=False, sentence_size=SENTENCE_SIZE)\n",
    "docs = loader.load_and_split(textsplitter)\n",
    "\n",
    "vs_path = \"./vector_storage\"\n",
    "faiss_vector_store = FAISS.from_documents(docs, hgf_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 124.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 支持哪些省份配送？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "faiss_vector_store.save_local(vs_path)\n",
    "\n",
    "faiss_vector_store = FAISS.load_local(vs_path, hgf_embeddings)\n",
    "query_faiss = \"配送范围是？\"\n",
    "result_docs = faiss_vector_store.similarity_search(query=query_faiss)\n",
    "print(result_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaodong/anaconda3/envs/chatglm/lib/python3.8/site-packages/langchain/chains/retrieval_qa/base.py:201: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import VectorDBQA\n",
    "\n",
    "# ChatGLM\n",
    "faq_chain = VectorDBQA.from_chain_type(llm=CustomLLM(), vectorstore=faiss_vector_store, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new VectorDBQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 60.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.THUDM.chatglm-6b-int4.02a065cf2797029c036a02cac30f1da1a9bc49a3.modeling_chatglm:The dtype of attention mask (torch.int64) is not bool\n",
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I'm sorry, but I don't have information on specific快递公司 or delivery times for locations like西藏. The delivery time for a specific order will depend on various factors such as the order's location, the service provider's network, and the delivery method used. Can you please provide more information about the order you are referring to so I can better assist you?\n"
     ]
    }
   ],
   "source": [
    "question = '''能送到西藏吗？大概需要几天？'''\n",
    "result = faq_chain.run(question)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
